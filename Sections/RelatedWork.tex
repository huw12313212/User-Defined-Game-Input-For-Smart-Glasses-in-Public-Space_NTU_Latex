\chapter{Related Work}
\label{c:related work}
\section{Game Input}

    There are many previous works exploring new kinds of game controls and pushing the limit of game design. Vickers et al.\cite{Vickers:2013:PLT:2531922.2514856} showed the possibility to use eye gestures as game inputs; Christian et al.\cite{Christian:2014:VSI:2559206.2580103} provided novel techniques for users to interact with games by head-gesture; 
    Harada et al.\cite{Harada:2011:VGI:2042053.2042059} and Sporka et al.\cite{Sporka:2006:NIS:1168987.1169023} both indicated that the voice input greatly expanded the scope of games that could be played hands-free and just counted on voice input; 
    Baba et al.\cite{Baba:2007:VGU:1278280.1278285} presented a game prototype which treated skin contact as controller input; Nacke et al.\cite{Nacke:2011:BGD:1978942.1978958} even considered using biofeedback (including EMG, EDA, EKG, RESP, TEMP) as game input methods;
    Hsu et al.\cite{Hsu:2014:GSE:2663204.2669623} compared different game inputs, including head gestures, voice control, handheld controller, joysticks, eye winking and glass touchpad, for First-Person Shooter(FPS) games on smart glasses. 


    \section{Mobile Input Technology}
    Some works related to mobile systems had defined designer-made input methods. These systems could be divided into two main categories, \emph{touch} and \emph{non-touch} inputs. 

    Harrison et al.\cite{Harrison:2011:OWM:2047196.2047255} created \textsl{OmniTouch}, a wearable depth-sensing and projected system that enables interactive multitouch applications on any surface of the user's body. Moreover, \textsl{Skinput}\cite{Harrison:2010:SAB:1753326.1753394}, a technology that appropriates the human body for acoustic transmission, and allows the skin to be used as an input surface. Chan et al. presented FingerPad\cite{Chan:2013:FPS:2501988.2502016}, a nail-mounted device that turns the tip of the index finger into a touchpad, allowing private and subtle interaction while on the move. Baudisch et al.\cite{Gustafson:2011:IPL:2047196.2047233} illustrated a concept of imaginary interface with sensing several gestures on the user's palms. Recently, Serrano et al.\cite{Serrano:2014:EUH:2611247.2556984} explored the use of \textsl{Hand-to-Face} input to interact with head-worn displays(HWD) and provided a set of guidelines for developing effective Hand-to-Face interactions based on two main factors they found, social acceptability and cultural effect.

    Kim et al.\cite{Kim:2012:DFI:2380116.2380139} developed a wrist-worn architecture, which supports discrete gesture recognition with reconstructing a 3D hand model in the air. Similarly, Jing et al.\cite{Jing:2013:MRS:2541831.2541875} implemented \textsl{Magic Ring}, a finger ring shaped input device using inertial sensors to detect subtle finger gestures; Cola\c{c}o et al.\cite{Colaco:2013:MCL:2501988.2502042} built a head-mounted display, \textsl{Mime}, sensing 3D gestures in front of the user's eyes. 

    \section{Gestures in HCI}
    Gesture-based interfaces are already common in a variety of application domains such as gaming, virtual or augmented reality and mobile devices\cite{karam2005taxonomy}. Aigner et al.\cite{aigner2012understanding} conducted a study of human preferences in usage of gesture types for HCI and indicated that, depending on the meaning of the gesture, there is preference in the usage of gesture types;
    Nielsen et al.\cite{nielsen2004procedure} pointed out some important issues in choosing the set of gestures for the interface from a user-centred view such as the learning rate, ergonomics, and intuition;
    Grijincu et al.\cite{Grijincu:2014:UIG:2669485.2669511} presented a video-based gesture dataset and a methodology for annotating video-based gesture datasets; 
    Recently, Piumsomboon et al.\cite{Piumsomboon:2013:UGA:2468356.2468527} have developed a user-defined gesture set for augmented reality applications. In our work, we focus on exploring relevant input gestures for gaming on smart glasses in public space.

    \section{User Elicitation Studies}
    User-elicitation studies are a specific type of participatory design methodology that involves end-users in the design of control-sets\cite{Morris:2012:WWI:2396636.2396651}. These studies had been used to design user interfaces of various types including multi-touch gestures on small and large surfaces\cite{Anthony:2012:IRC:2396636.2396671,Wobbrock:2009:UGS:1518701.1518866} and multi-modal interactions \cite{Morris:2012:WWI:2396636.2396651,Liang:2012:USG:2350046.2350098}. There is also some evidence that user-defined control sets are more complete than those sets defined solely by experts\cite{Pyryeskin:2012:CEG:2396636.2396638,Wobbrock:2009:UGS:1518701.1518866}.

    In a user-elicitation study, users were shown referents (an action's effects) and were asked to demonstrate the interactions that resulted in a given referent\cite{Wobbrock:2009:UGS:1518701.1518866}. In this work, we draw upon the user-elicitation methodology to identify user expectations and suggestions for smart glass gaming.
